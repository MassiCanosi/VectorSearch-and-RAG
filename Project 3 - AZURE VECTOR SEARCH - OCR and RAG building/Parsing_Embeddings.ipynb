{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "857fd4f8",
   "metadata": {},
   "source": [
    "# <span style=\"color: yellow\"> OCR, Embedding, Index Creation, and Testing </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import io\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# ocr imports\n",
    "import pytesseract\n",
    "# import pymupdf\n",
    "from PyPDF2 import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "import fitz\n",
    "from fitz import open as fitz_open\n",
    "import win32com.client\n",
    "\n",
    "# API & Embeddings\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "\n",
    "# db management imports\n",
    "import pymongo\n",
    "import requests\n",
    "\n",
    "# dotenv imports\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "############################################\n",
    "\n",
    "user = '___insert_user_here___'\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = fr\"c:\\Users\\{user}\\AppData\\Local\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(document):\n",
    "    replacements = [(\"\\r\", \"\"), (\"\\x07\", \"\"), (\"_\", \" \"), (r\"\\s+\", \" \")]\n",
    "    for pat, repl in replacements:\n",
    "        document = re.sub(pattern=pat, repl=repl, string=document)\n",
    "\n",
    "    return document.strip()\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "def extract_text_from_doc(temp_path):\n",
    "    \"\"\"\n",
    "    Parse a doc/docx file\n",
    "    \"\"\"\n",
    "\n",
    "    word = win32com.client.Dispatch(\"Word.Application\")\n",
    "    word.Visible = False\n",
    "    doc = word.Documents.Open(temp_path)\n",
    "    full_text = []\n",
    "\n",
    "    # Legge tutto il testo del documento\n",
    "    for para in doc.Paragraphs:\n",
    "        full_text.append(para.Range.Text.strip() + \"\\n\")\n",
    "\n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "\n",
    "    cleaned_text = [page for page in full_text if isinstance(page, str)]\n",
    "    document = \" \".join(cleaned_text)\n",
    "\n",
    "    return document\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "def parse_file(temp_path, fileName):\n",
    "    \"\"\" \n",
    "    Parse a PDF or a doc/docx file and return a dictionary with the text of each page.\n",
    "    1. DOC/DOCX parsing: extract_text_from_doc(filepath)\n",
    "    2. PDF parsing:\n",
    "        -- First try (page.extract_text_from_doc): get text from the pdf page. If any text is retrieved with a number of characters > 20, it will be appended to \"blocks\". \n",
    "        -- Second try (pytesseract.image_to_string): if the text is not being retrieved from the first try block, use OCR to parse the image. then append the result to \"blocks\".\n",
    "    \"\"\"\n",
    "\n",
    "    blocks = \"\"\n",
    "\n",
    "    file_json = dict()\n",
    "    file_json[fileName] = dict()\n",
    "\n",
    "    ##\n",
    "\n",
    "    if (fileName.endswith(\".doc\")) or (fileName.endswith(\".docx\")):\n",
    "\n",
    "        document = extract_text_from_doc(fileName)\n",
    "        clean_document = clean_text(document)\n",
    "\n",
    "        file_json[fileName]['text'] = clean_document\n",
    "\n",
    "    else:\n",
    "\n",
    "        document = PdfReader(temp_path)\n",
    "        page_images = convert_from_path(temp_path, dpi=300)\n",
    "        \n",
    "        for i, page in enumerate(document.pages): \n",
    "            page_text = page.extract_text().replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "            if page_text and len(page_text.strip())> 20:\n",
    "                clean_page_text = clean_text(page_text)\n",
    "                blocks += clean_page_text + \"\\n\"\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    ocr_text = pytesseract.image_to_string(page_images[i], lang='ita+eng')\n",
    "                    blocks += ocr_text + \"\\n\"\n",
    "\n",
    "                except Exception as e:\n",
    "                            print(f\"[OCR error {i+1}] {str(e)}\")\n",
    "\n",
    "        file_json[fileName]['text'] = blocks\n",
    "\n",
    "    return file_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd57f3",
   "metadata": {},
   "source": [
    "## Microsoft Graph file selection & Token refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_token():\n",
    "    token_url = f\"___token_url___\"\n",
    "    resource_url = \"https://graph.microsoft.com/.default\"\n",
    "\n",
    "    response = requests.post(token_url, data={\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": \"___clientid___\",\n",
    "        \"client_secret\": \"____clientsecret____\",\n",
    "        \"scope\": resource_url\n",
    "    })\n",
    "\n",
    "\n",
    "    access_token = response.json().get(\"access_token\")\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "    return headers\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "def retrieve_id(id, folder_ids, headers):\n",
    "\n",
    "    \"\"\"\n",
    "    Recursive function to find all the files at any depth inside MS Graph\n",
    "    \"\"\"\n",
    "\n",
    "    driveId = '____drive_id____'\n",
    "\n",
    "    url_2 = f'https://graph.microsoft.com/v1.0/drives/{driveId}/items/{id}/children/'\n",
    "\n",
    "    site_info_2 = requests.get(url_2, headers=headers).json()\n",
    "\n",
    "    for dic in site_info_2['value']:\n",
    "\n",
    "        folder_key = dic.keys()\n",
    "\n",
    "        if 'folder' in folder_key:\n",
    "            name = dic['name']\n",
    "            id = dic['id'] # FOLDER\n",
    "\n",
    "            retrieve_id(id, folder_ids, headers)\n",
    "        \n",
    "        elif 'file' in folder_key:\n",
    "            webUrl = dic['webUrl']\n",
    "            name = dic['name']\n",
    "            id = dic['id'] # FILE\n",
    "\n",
    "            folder_ids.append({'id': id, 'name': name, 'webUrl': webUrl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163607af",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = refresh_token()\n",
    "\n",
    "sharepoint_site = \"inoffice.sharepoint.com\"\n",
    "site_path = \"____sitepath____\"\n",
    "site_info_url = f\"https://graph.microsoft.com/v1.0/sites/{sharepoint_site}:{site_path}\"\n",
    "site_info = requests.get(site_info_url, headers=headers).json()\n",
    "site_id = site_info[\"id\"]\n",
    "\n",
    "drives_url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives\"\n",
    "drives = requests.get(drives_url, headers=headers).json()\n",
    "\n",
    "folderId = '____folder_id____' # PARENT FOLDER \n",
    "folder_ids = list()\n",
    "retrieve_id(folderId, folder_ids, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62725490",
   "metadata": {},
   "source": [
    "### Keep only .pdf, .doc, .docx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b8c8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = dict()\n",
    "\n",
    "for dic in folder_ids:\n",
    "    if (dic['name'].endswith('.pdf') or dic['name'].endswith('doc') or dic['name'].endswith('docx')):\n",
    "            id = dic['id']\n",
    "            valid[id] = dict()\n",
    "            valid[id]['name'] = dic['name']\n",
    "            valid[id]['webUrl'] = dic['webUrl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2860efb",
   "metadata": {},
   "source": [
    "### Retrieve text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = refresh_token()\n",
    "\n",
    "drive_id = '___driveid___'\n",
    "full_json = list()\n",
    "\n",
    "iterator = [x for x in range(0,4)] #set here the number of files you want to test the model on. Set len(files) to parse all.\n",
    "\n",
    "for i, (k,v) in zip(iterator, valid.items()):\n",
    "\n",
    "    full_text = \"\"\n",
    "\n",
    "    fileName = v['name']\n",
    "    extension = fileName.rsplit(\".\")[0]\n",
    "    filepath = v['webUrl']\n",
    "\n",
    "    print(fileName)\n",
    "\n",
    "    download_file_reference = filepath.replace(\"https://inoffice.sharepoint.com/sites/AAAA/BBBB/\", \"\")\n",
    "\n",
    "    download_url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:/{download_file_reference}:/content\"\n",
    "    response = requests.get(download_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        file_bytes = BytesIO(response.content)\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=extension) as temp_file:\n",
    "            try:\n",
    "                temp_file.write(file_bytes.getvalue())\n",
    "                temp_path = temp_file.name\n",
    "\n",
    "                file_json = parse_file(temp_path, fileName)\n",
    "\n",
    "                file_json[fileName]['reference_link'] = filepath\n",
    "\n",
    "                full_json.append(file_json)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"ERRORE:\", e)\n",
    "            \n",
    "            finally:\n",
    "                temp_file.close()\n",
    "                os.unlink(temp_path)\n",
    "    \n",
    "    else:\n",
    "        print(\"Response:\", response, response.status_code)\n",
    "\n",
    "    print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c573fdf",
   "metadata": {},
   "source": [
    "### Store the file for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = r'___insert_path_here___'\n",
    "\n",
    "with open(path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(full_json, file)\n",
    "\n",
    "##\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    temp_full_json_import = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import unicodedata\n",
    "\n",
    "###\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "###\n",
    "\n",
    "def split_text_smart(text, max_chars=3000):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    parts = re.split(r'(\\n\\n+)', text)\n",
    "\n",
    "    for part in parts:\n",
    "        if len(current_chunk) + len(part) <= max_chars:\n",
    "            current_chunk += part\n",
    "        else:\n",
    "            if len(part) > max_chars:\n",
    "                sub_parts = re.split(r'(?<=[.!?])\\s+', part)\n",
    "                for sub in sub_parts:\n",
    "                    if len(current_chunk) + len(sub) <= max_chars:\n",
    "                        current_chunk += sub + \" \"\n",
    "                    else:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                        current_chunk = sub + \" \"\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = part\n",
    "\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key='___apikey___',\n",
    "    api_version='___apiversion___',\n",
    "    azure_endpoint='___azureendpoint___'\n",
    "    )\n",
    "\n",
    "deployment_name = 'gpt-4o'\n",
    "\n",
    "PROMPT_TEMPLATE = \n",
    "\"\"\"\n",
    "    You will receive text generated by an OCR system. The text may contain symbols, meaningless characters, unreadable fragments, or incomplete sentences.\n",
    "\n",
    "    Your task is to:\n",
    "\n",
    "    Remove all text that doesn't make sense or is clearly the result of an OCR error.\n",
    "\n",
    "    Keep only the readable, coherent, and useful text.\n",
    "\n",
    "    ** Do not add, complete, or invent anything **. If a sentence is incomplete or incomprehensible, remove it.\n",
    "\n",
    "    Return only the cleaned text, without comments or explanations.\n",
    "\n",
    "    Here is the text to clean:\n",
    "    ---\n",
    "    {OCR_TEXT}\n",
    "\"\"\"\n",
    "\n",
    "def clean_output(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def process_chunks(chunks, error_lst):\n",
    "    full_cleaned_text = \"\"\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        prompt = PROMPT_TEMPLATE.format(OCR_TEXT=chunk)\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=deployment_name,\n",
    "                temperature=0,\n",
    "                max_tokens=4000,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an assistant that cleans OCR texts by removing unnecessary content. Do not add anything.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            cleaned = response.choices[0].message.content\n",
    "            cleaned = clean_output(cleaned)\n",
    "            full_cleaned_text += cleaned + \" \"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_lst.append(chunk)\n",
    "            full_cleaned_text += chunk + \" \"\n",
    "            print(f\"Errore nel chunk {i+1}: {e}\")\n",
    "\n",
    "    return full_cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df8b94",
   "metadata": {},
   "source": [
    "### Text formatting and chunks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, nested_dic in enumerate(temp_full_json_import):\n",
    "        for k,v in nested_dic.items():\n",
    "                error_lst = list()\n",
    "\n",
    "                print(k)\n",
    "\n",
    "                text = v['text']\n",
    "\n",
    "                v['text'] = normalize_unicode(text)\n",
    "\n",
    "                v['text'] = re.sub(pattern=r'[\\x00-\\x1F\\x7F-\\x9F]', repl='', string=v['text'])\n",
    "\n",
    "                chunks = split_text_smart(v['text'])\n",
    "\n",
    "                v['openAI text'] = process_chunks(chunks, error_lst)\n",
    "\n",
    "                v['errors'] = error_lst\n",
    "\n",
    "                print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92b3b4",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80815bc",
   "metadata": {},
   "source": [
    "## RAG Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00322f",
   "metadata": {},
   "source": [
    "### Refresh client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f32268",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client = os.getenv('myclient')\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version = \"apiversion\",\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "tk_encoding = tiktoken.encoding_for_model(\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b521e",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "76894a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"deployment_name\"\n",
    "def generate_embeddings(tk_encoding, text, n_tokens=8191, token_overlap=30, model=\"text-embedding-ada-002\"): \n",
    "\n",
    "    text_tokens = tk_encoding.encode(text) # list containing text tokens\n",
    "    chunks = [tk_encoding.decode(text_tokens[i:i+n_tokens]) for i in range(0, len(text_tokens), n_tokens - token_overlap)] # decode encoded text, from i to i+8191 iteratively with a step of 8161\n",
    "\n",
    "    embeddings = list()\n",
    "    chunked_text = list()\n",
    "\n",
    "    for chunk in chunks:\n",
    "        embeddings.append(client.embeddings.create(input=chunk, model=model).data[0].embedding)\n",
    "        chunked_text.append(chunk)\n",
    "\n",
    "    return chunked_text, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03eb68a",
   "metadata": {},
   "source": [
    "### File filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61e4077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(temp_full_json_import):\n",
    "    fileName = list(v.keys())[0]\n",
    "    openAI_text = v[fileName]['openAI text']\n",
    "    chunked_text, embeddings = generate_embeddings(tk_encoding, openAI_text)\n",
    "    v[fileName]['chunked_text'] = chunked_text\n",
    "    v[fileName]['embedded_text'] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ed074",
   "metadata": {},
   "source": [
    "## <span style=\"color: yellow\"> Azure embedded data ingestion </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "778472d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ComplexField,\n",
    "    CorsOptions,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    ")\n",
    "\n",
    "from azure.search.documents.indexes.models import (\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc691d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.pipeline.transport import RequestsTransport\n",
    "\n",
    "azure_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "azure_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "\n",
    "# Disabilita SSL verify nel transport\n",
    "transport = RequestsTransport(connection_verify=False)\n",
    "\n",
    "azure_client = SearchIndexClient(azure_service_endpoint, AzureKeyCredential(azure_key), transport=transport)\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=azure_service_endpoint,\n",
    "    index_name=\"complianceindex\",\n",
    "    credential=AzureKeyCredential(azure_key),\n",
    "    # override qui\n",
    "    transport=transport  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "31d8fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_creation():\n",
    "    name = \"complianceindex\"\n",
    "    fields = [\n",
    "        SimpleField(name=\"FileId\", type=SearchFieldDataType.String, key=True),\n",
    "        SimpleField(name=\"webUrl\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"FileName\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "        SearchableField(name=\"text\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "        SearchField(name=\"embedded_text\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "                        searchable=True, \n",
    "                        vector_search_dimensions=1536,\n",
    "                        vector_search_profile_name=\"simple-vector-config\"\n",
    "                        )\n",
    "    ]\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"simple-vector-config\",\n",
    "                algorithm_configuration_name=\"simple-algorithms-config\",\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"simple-algorithms-config\",\n",
    "                kind=\"hnsw\",\n",
    "                parameters={\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\",\n",
    "                    },\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    result = azure_client.create_index(index)\n",
    "\n",
    "    return result\n",
    "\n",
    "###\n",
    "\n",
    "def get_index():\n",
    "    name = \"complianceindex\"\n",
    "    result = azure_client.get_index(name)\n",
    "    return result\n",
    "\n",
    "###\n",
    "\n",
    "def upload_document(document):\n",
    "    result = search_client.upload_documents(documents=document)\n",
    "    print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "    return result\n",
    "\n",
    "###\n",
    "\n",
    "def create_structure(accum: int, fileName, webUrl, chunked_text, embedded_text):\n",
    "\n",
    "    structure = {\n",
    "            \"FileId\": str(accum),\n",
    "            \"FileName\": fileName,\n",
    "            \"webUrl\": webUrl,\n",
    "            \"text\": chunked_text, \n",
    "            \"embedded_text\": embedded_text\n",
    "        }\n",
    "\n",
    "    return structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacf894",
   "metadata": {},
   "source": [
    "### Structure Creation for List to Upload on Azure Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = 0 \n",
    "lst_to_upload = list()\n",
    "\n",
    "for i, v in enumerate(temp_full_json_import):\n",
    "    fileName = list(v.keys())[0]\n",
    "    fileName_noExtension = list(v.keys())[0].rsplit(\".\")[0]\n",
    "\n",
    "    text = v[fileName]['openAI text']\n",
    "    webUrl = v[fileName]['reference_link']\n",
    "    chunked_text = v[fileName]['chunked_text']\n",
    "\n",
    "    for i, chunk in enumerate(chunked_text):\n",
    "\n",
    "        embedded_text = v[fileName]['embedded_text'][i]\n",
    "        lst_elem = create_structure(accum, fileName_noExtension, webUrl ,chunk, embedded_text)\n",
    "        lst_to_upload.append(lst_elem)\n",
    "        ##\n",
    "        print(colored(f\"â–¶ Chunk {accum} created for {fileName}\\n***\", color=\"yellow\", attrs=[\"bold\"]))\n",
    "        ##\n",
    "        accum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15c9e3",
   "metadata": {},
   "source": [
    "### Index creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc28fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_creation()\n",
    "result = upload_document(lst_to_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bf2e7",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search content inside Azure index based on the user query\n",
    "    \"\"\"\n",
    "    results = search_client.search(query, top=top_k)\n",
    "    output = [doc['text'] for doc in results]\n",
    "    return output\n",
    "\n",
    "############\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def chatbot_with_context(user_input):\n",
    "    \"\"\"#\n",
    "    Use this function for testing purposes. The actual function is inside the python backend.\n",
    "    \"\"\"\n",
    "    search_results = search_documents(user_input)\n",
    "    context = \"\\n\\n\".join(search_results)\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "                    You are an AI assistant. Respond only based on the documents below.\n",
    "                    If an answer is not contained in the documents, say that you are not sure.\n",
    "                    Documents:\n",
    "                    {context}\n",
    "                    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + chat_history\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.3,\n",
    "        max_tokens=800\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return reply"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
